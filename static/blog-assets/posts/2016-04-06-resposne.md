---
date: 2016-04-06
title: "Author’s Response to Stephen Few’s critique of Hypothetical Outcome Plots"
---
Hypothetical outcome plots (HOPs) are an approach to visualizing uncertainty using a set of discrete outcomes. HOPs consist of a set of frames, each depicting a draw from a theoretical distribution, which are presented using animation or small multiples. HOPs can act as an alternative to static representations including error bars, standard pdf plots, violin plots or gradient plots or provide an additional layer of information on such plots. To learn more about HOPs, consult our [paper and related materials](http://journals.plos.org/plosone/article?id=10.1371%2Fjournal.pone.0142444) or [blog post](https://medium.com/hci-design-at-uw/hypothetical-outcomes-plots-experiencing-the-uncertain-b9ea60d7c740).

HOPs may be especially useful for more complex plots, for which there is no standard static representation of uncertainty. Our recent paper, however, focused on simple one-, two-, and three-variable outcome plots. It provided empirical evidence from an experiment showing that untrained users are able to interpret HOPs to assess probabilities as well as or better than they are able to interpret error bars or violin plots.

Recently, Stephen Few, a visualization practitioner, [critiqued](http://www.perceptualedge.com/blog/?p=2275) the experiments in our paper. Two major arguments about the validity of our study were made in the critique:

1. *Appropriateness of task*: The inferences that subjects’ made about one, two and three variable plots in the study were not representative of the inferences that people make or are supposed to make with visualizations of distributions. Instead, the study was designed to favor HOPs because subjects report discrete numbers which can be counted with HOPs but not with error bars or violin plots.

2. *Appropriateness of representation*: The probability questions that subjects were asked could be better answered by a different representation (at the extreme, presenting only a number).

We take this opportunity to clarify why the results of our experiments are valid and to reflect on the uses for, and evaluation of, uncertainty visualizations.

*Critique 1: The inferences that subjects made about one, two and three variable plots in the study were not representative of the inferences that users make, or are supposed to make, with visualizations of distributions.*

*Our Response*: We agree with Few that distribution plots for single random variables are often used to make inferences about central tendency, spread, and shape, such as in exploratory analysis. However, we disagree that these are the only inferences people make from distribution plots. It is also common to use distribution plots to make rough estimates of probabilities for one and two-sided intervals (properties of the cumulative distribution function).

Consider, for example, the common case in the scientific literature where an author presents uncertainty visualizations like error bars or suggested alternatives like violin plots or gradient plots ([Correll and Gleicher 2014](http://graphics.cs.wisc.edu/Vis/ErrorBars/)) in reporting analysis results. The error bar is included to allow a viewer to get a sense of the dispersion of hypothetical statistics (e.g., sample means); in other words, the error bar shows information about the sampling distribution. This distributional information is intended to allow the user to assess the reliability of the plotted statistic. That is, the viewer should be able to make a rough judgment about the probability that the result would be above or below a threshold, or within a given interval, upon replication.

Few also suggests that when a joint distribution of multiple random variables is presented, comparative judgments (e.g., pr(B > A)) are not common. We disagree. In fact, it is quite common in many real world tasks. For example, we might use collected data to predict the probability that one bus will reach our bus stop sooner than another, that one athlete will run faster than another in a race, or that one stock will have a higher value at a predetermined sell date. There are also analogous non-prediction tasks similar to the question we asked. For example, given historical data, was bus A or bus B more likely to arrive at stop Y first. A natural extension to the tasks we used in our study incorporates effect size in addition to ordering. For example, we could ask the subject to estimate the probability that replicating the experiment would show an effect at least as big as some threshold.

In the scientific literature, when multiple error bars represent the dispersion of sampling distributions, the visualizations are also intended to enable comparative judgments. For example, what is the probability that a draw from distribution B (the sampling distribution for the mean of the treatment condition) will be larger than a draw from distribution A (the sampling distribution for the control condition)? The error bars are meant to give a viewer a way to intuitively assess such probabilities. Unfortunately, most people, including scientific researchers, fail to make such comparisons accurately with conventional representations of confidence intervals like error bars ([Belia et al. 2005](http://isites.harvard.edu/fs/docs/icb.topic477909.files/misunderstood_confidence.pdf)). These issues, among others, have led statistical reformers to call for new representations and tools that make it easier for people to assess reliability without relying solely on hypothesis tests ([Cumming 2012](https://www.routledge.com/products/9780415879682)).

*Critique 1, continued: The study was designed to favor HOPs because subjects report discrete numbers which can be counted with HOPs but not with error bars or violin plots.*

Evaluations typically operationalize performance as accuracy — how close the user’s inference is to some verifiable property of the data. For evaluating an uncertainty visualization, we think that the appropriate verifiable properties are probabilities of specific outcomes (e.g., the probability that a randomly selected student scored above 80; the probability that a randomly selected student from class A scored higher than a randomly selected student from B; the probability that the mean grade in class A would be higher than in class B if the exam were repeated).

An alternative to asking subjects to report a probability is to have subjects view the uncertainty representation, report some property, and then express how confident they are about the report. This approach is problematic because there is no correct amount of confidence to report, and consequently no way to assess the correctness of responses about individual plots. It is only possible to infer that subjects are reading multiple plots inconsistently. That method is therefore less precise than asking questions with verifiable answers, as we did.

We frame the probability questions in our study as frequencies (how many times out of 100) rather than as probabilities. Using a frequency framing to elicit probabilities has been shown to improve people’s abilities to engage in Bayesian reasoning over directly asking for probabilities ([Gigerenzer and Hoffrage 1995](http://www.cogsci.ucsd.edu/~coulson/203/GG_How_1995.pdf)), thereby reducing the noisiness of estimates. While HOPs users may count to reach their estimates, we do not know if this is the best characterization of what most users do, or the most successful strategy for using HOPs.

*Critique 2: The probability questions that subjects were asked could be better answered by presenting an alternative visualization designed for that query.*

*Our Response*: Taken to the extreme, a number that distills the target property into a single measure will always be the best representation for conveying that single property. However, well-designed visualizations represent data efficiently in ways that enable multiple inferences. Clearly, providing highly tailored visualizations or the answers in numeric form will make it easier for users to answer specific questions, but the fair assessment is whether users can make inferences from the generic visualizations.

Thus, for example, if we only wanted users to be able to identify the mean score of students on an exam, it would be best to just plot that mean as a single dot, or even to provide a number with no visualization. To enable inference of only the percentage of the students who scored higher than 80, again a single number will outperform any visualization. If, on the other hand, we want users to be able to estimate the percentage of students who scored higher than all possible cutoffs *k*, then we might design a visualization such as an error bar or violin plot or HOPs with the goal of supporting these inferences. To assess whether the visualization works, it is reasonable to conduct a test of subjects’ ability to answer the question for varying cutoffs.

Of course, there are still legitimate questions for future research about how HOPs are used and where they work best, including whether HOPs are useful in an analysis setting compared to other static or interactive representations, and by what exact mechanism they work (e.g., counting, integration through ensemble processing, etc). Our recent work provides clear empirical evidence that untrained users can use HOPs in order to read off properties of cumulative distribution functions and comparative properties of joint distributions. We suspect that HOPs will turn out to be even more useful for more complex plots, where there is no standard static representation of uncertainty. We hope others will join us in conducting further research on representations of uncertainty, including how interactive selections and hybrid visualizations that combine dynamic and static depictions support reliability judgments.

Factual clarifications:

1. Few refers to HOPs as our invention. This is not quite accurate. Animated HOPs or HOPs-like visualizations have appeared elsewhere in education, research, and the media. For example, [“the dance of the means”](https://www.youtube.com/watch?v=iJ4kqk3V8jQ) is used to support intuitions for inferential statistics like confidence intervals ([Cumming 2012](https://www.routledge.com/products/9780415879682)). Animated hypothetical outcomes have been used to visualize uncertainty in geospatial data ([Fisher 1994](http://cat.inist.fr/?aModele=afficheN&cpsidt=4262227), [Ehlschlaeger et al. 1997](https://www.researchgate.net/profile/Charles_Ehlschlaeger/publication/222051179_Visualizing_spatial_data_uncertainty_using_animation/links/5400bf590cf23d9765a44f8a.pdf), [Bastin et al. 2002](http://www.sciencedirect.com/science/article/pii/S0098300401000516)). HOPs-like visualizations also appear in recent interactives by Amanda Cox and others at the New York Times (e.g., http://goo.gl/Oiq3W6, http://goo.gl/Df8orO, http://goo.gl/PdBxfY). Our work is the first to describe and study HOPs as a generalizable technique for presenting uncertainty. This includes identifying design requirements for applying HOPs to simple plots of random variables as well as more complex data forms and visualizations (e.g., clustered network diagrams, geospatial visualizations, tree diagrams), and evaluating how inferences based on HOPs compare to those made with static representations.

2. Few reports that in our study, the animated HOPs that subjects are shown contain approximately 100 frames randomly sampled from 5000 generated draws. This is not accurate. Each animated HOPs in our study was generated to display all 5000 draws as individual frames. The same 5000 draws was used to generate the error bars and violin plots. Few may be referring to the median number of frames that we estimate to have been displayed from the time a HOPs subject loaded the page to the time they submitted their response(s).

*This post was authored by Jessica Hullman, in collaboration with Paul Resnick and Eytan Adar. Thanks also to Jeffrey Heer and Matthew Kay for feedback.*
